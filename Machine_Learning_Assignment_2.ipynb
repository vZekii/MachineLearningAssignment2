{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWQ8e3nWVrt08BRv0CEZio",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vZekii/MachineLearningAssignment2/blob/main/Machine_Learning_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task - Something something model or implementation\n",
        "\n",
        "## Introduction\n",
        "Discuss the definition, training and motivations of the model/study\n",
        "\n",
        "\n",
        "I will do a combination of the 2 below tasks:\n",
        "### Task 1\n",
        "Implement a Perceptron 2-class classification model from scratch, ensuring that you cover all the essential components. This includes:\n",
        "\n",
        "Score Computation: Implement the computation of the weighted sum of the inputs. You may want to investigate how to process multiple samples in one call of functions using `numpy`, `pytorch` libraries.\n",
        "\n",
        "Classification Rule: Define the rule that maps the computed score to one of the two classes.\n",
        "\n",
        "Parameter Updating Rule: Implement the rule for updating the weights and bias in response to the training data. (We will introduce this step shortly, but the implementation is straightforward, you can easily complete via help with ChatGPT/google)\n",
        "\n",
        "Further Consideration: Pay particular attention to scenarios where the data is not linearly separable. Analyse how your implementation behaves in these cases and discuss any strategies you might use to handle them.\n",
        "\n",
        "### Task 2\n",
        "Parameter Sensitivity: Build upon the basic individual-sample update rule, we can consider summing up the update signals from multiple samples (a training batch), but each time with a certain step-size (learning rate, implemented in the given notebook). Study how the perceptron model behaves with different hyperparameters like learning rate and batch size. How do these parameters affect the speed and stability of learning?\n",
        "\n",
        "# Task definition\n",
        "I will implement a 2-class perceptron in python, including its functionalities of Score Computation, classification rules, parameter updating, while also analysing cases where data is not linearly seperable and what strategies can be used to overcome this"
      ],
      "metadata": {
        "id": "eFiX7pgr8aNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have asked chat gpt to generate some initial code for this task"
      ],
      "metadata": {
        "id": "m8aDGUADGKrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import Required Libraries:\n",
        "Start by importing the necessary libraries, including scikit-learn to load the Iris dataset and NumPy for numerical operations."
      ],
      "metadata": {
        "id": "nDAMsBh4GSld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "YE49j4-LGKcA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load and Prepare the Iris Dataset:\n",
        "Load the Iris dataset and split it into training and testing sets."
      ],
      "metadata": {
        "id": "hNoCSX4AGU5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "KNhWmP16GWZg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement the Multi-Class Perceptron:\n",
        "Create a class for the multi-class perceptron. The perceptron will have weights and biases for each class."
      ],
      "metadata": {
        "id": "GECEEznFGXv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiClassPerceptron:\n",
        "    def __init__(self, num_classes, num_features, learning_rate=0.1, num_epochs=100):\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = num_features\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.weights = np.zeros((num_classes, num_features))\n",
        "        self.biases = np.zeros(num_classes)\n",
        "\n",
        "    def predict(self, x):\n",
        "        scores = np.dot(self.weights, x) + self.biases\n",
        "        return np.argmax(scores)\n",
        "\n",
        "    def train(self, X, y):\n",
        "        for epoch in range(self.num_epochs):\n",
        "            for i in range(len(X)):\n",
        "                x = X[i]\n",
        "                y_true = y[i]\n",
        "\n",
        "                y_pred = self.predict(x)\n",
        "                if y_pred != y_true:\n",
        "                    self.weights[y_true] += self.learning_rate * x\n",
        "                    self.biases[y_true] += self.learning_rate\n",
        "                    self.weights[y_pred] -= self.learning_rate * x\n",
        "                    self.biases[y_pred] -= self.learning_rate\n",
        "\n",
        "    def train_batch(self, X, y):\n",
        "      \"\"\"\n",
        "      chatgpt implementation of a training batch\n",
        "      \"\"\"\n",
        "      for epoch in range(self.num_epochs):\n",
        "          # Initialize updates for weights and biases\n",
        "          weight_updates = np.zeros(self.weights.shape)\n",
        "          bias_updates = np.zeros(self.biases.shape)\n",
        "\n",
        "          for i in range(len(X)):\n",
        "              x = X[i]\n",
        "              y_true = y[i]\n",
        "\n",
        "              y_pred = self.predict(x)\n",
        "              if y_pred != y_true:\n",
        "                  weight_updates[y_true] += x\n",
        "                  bias_updates[y_true] += 1\n",
        "                  weight_updates[y_pred] -= x\n",
        "                  bias_updates[y_pred] -= 1\n",
        "\n",
        "          # Update weights and biases with accumulated updates\n",
        "          self.weights += self.learning_rate * weight_updates\n",
        "          self.biases += self.learning_rate * bias_updates"
      ],
      "metadata": {
        "id": "eaFr2FEjGZfB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Train the Multi-Class Perceptron:\n",
        "Instantiate the multi-class perceptron, and train it on the training data."
      ],
      "metadata": {
        "id": "Ts9pNfXUGc3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(np.unique(y_train))\n",
        "num_features = X_train.shape[1]\n",
        "perceptron = MultiClassPerceptron(num_classes, num_features)\n",
        "train_multi = True\n",
        "\n",
        "if not train_multi:\n",
        "  # Use standard single-updating method\n",
        "  perceptron.train(X_train, y_train)\n",
        "\n",
        "else:\n",
        "  # Modify your training loop to use batches\n",
        "  batch_size = 10  # Adjust the batch size as needed\n",
        "  num_epochs = 100\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      for i in range(0, len(X_train), batch_size):\n",
        "          X_batch = X_train[i:i+batch_size]\n",
        "          y_batch = y_train[i:i+batch_size]\n",
        "\n",
        "          perceptron.train_batch(X_batch, y_batch)\n"
      ],
      "metadata": {
        "id": "kyw-VkP2GbTz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Evaluate the Model:\n",
        "Test the model on the testing data and calculate the accuracy."
      ],
      "metadata": {
        "id": "1i90bMO6GeKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "standard_algo = False\n",
        "if standard_algo:\n",
        "  correct = 0\n",
        "  for i in range(len(X_test)):\n",
        "      if perceptron.predict(X_test[i]) == y_test[i]:\n",
        "          correct += 1\n",
        "\n",
        "  accuracy = correct / len(X_test)\n",
        "  print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "else:\n",
        "  # Implement hyperparameter optimisation\n",
        "  learning_rates = [0.1, 0.01, 0.001]\n",
        "  batch_sizes = [10, 32, 64]\n",
        "  num_epochs = [50, 100, 200]\n",
        "  best_accuracy = 0\n",
        "  best_hyperparameters = {}\n",
        "\n",
        "  for lr in learning_rates:\n",
        "      for batch_size in batch_sizes:\n",
        "          for epochs in num_epochs:\n",
        "              perceptron = MultiClassPerceptron(num_classes, num_features, learning_rate=lr, num_epochs=epochs)\n",
        "\n",
        "              for epoch in range(epochs):\n",
        "                  for i in range(0, len(X_train), batch_size):\n",
        "                      X_batch = X_train[i:i+batch_size]\n",
        "                      y_batch = y_train[i:i+batch_size]\n",
        "\n",
        "                      perceptron.train_batch(X_batch, y_batch)\n",
        "\n",
        "              correct = 0\n",
        "              for i in range(len(X_test)):\n",
        "                  if perceptron.predict(X_test[i]) == y_test[i]:\n",
        "                      correct += 1\n",
        "\n",
        "              accuracy = correct / len(X_test)\n",
        "\n",
        "              if accuracy > best_accuracy:\n",
        "                  best_accuracy = accuracy\n",
        "                  best_hyperparameters = {'learning_rate': lr, 'batch_size': batch_size, 'num_epochs': epochs}\n",
        "\n",
        "  print(f\"Best Hyperparameters: {best_hyperparameters}\")\n",
        "  print(f\"Best Accuracy: {best_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJc1zM8VGf5i",
        "outputId": "e7df5745-f717-4475-e60f-2b223f037eca"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 50}\n",
            "Best Accuracy: 100.00%\n"
          ]
        }
      ]
    }
  ]
}